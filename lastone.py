# -*- coding: utf-8 -*-
"""lastone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PsgjSsy2si1GsbM1UYDJmtwOEekP0pcC
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.decomposition import PCA
from sklearn.utils import resample
from google.colab import files

# Upload and Load Dataset
uploaded = files.upload()
file_path = list(uploaded.keys())[0]
dataset = pd.read_csv(file_path)

print("\n--- Dataset Description ---\n")
print(dataset.describe())

print("\n--- Dataset Info ---\n")
print(dataset.info())

dataset.head()

#  Handle Missing Values
print("\nChecking for missing values...")
print(dataset.isnull().sum())
dataset = dataset.dropna()

# Data Cleaning and Feature Engineering
dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])
dataset['is_organic'] = dataset['waste_type'].apply(lambda x: 1 if x == 'organic' else 0)
dataset_cleaned = dataset.drop(columns=['sensor_id', 'timestamp', 'waste_type'])

# Feature Scaling
numeric_columns = ['inductive_property', 'capacitive_property', 'moisture_property', 'infrared_property']
scaler = StandardScaler()
dataset_cleaned[numeric_columns] = scaler.fit_transform(dataset_cleaned[numeric_columns])

# Pairplot
import seaborn as sns
import matplotlib.pyplot as plt
sns.pairplot(dataset)
plt.show()

# Heatmap of Correlation
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Filter out non-numeric columns
numeric_dataset = dataset.select_dtypes(include=['float64', 'int64'])

# Plot heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(numeric_dataset.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

#  Correlation Matrix
dataset.hist(figsize=(10, 6))
plt.tight_layout()
plt.show()

# Step 4: Scatter plot
plt.figure(figsize=(10, 5))
sns.scatterplot(data=dataset, x='inductive_property', y='infrared_property', hue='waste_type')
plt.title('Scatter plot: Inductive Property vs Infrared Property')
plt.xlabel('Inductive Property')
plt.ylabel('Infrared Property')
plt.show()

# Bar plot
plt.figure(figsize=(10, 6))
sns.countplot(data=dataset, x='is_organic', palette='Set1')
plt.title('Organic vs Non-Organic Waste')
plt.xlabel('Is Organic')
plt.ylabel('Count')
plt.show

# Balancing the Dataset
X = dataset_cleaned.drop(columns=['is_organic'])
y = dataset_cleaned['is_organic']
data = pd.concat([X, y], axis=1)
majority = data[data['is_organic'] == 0]
minority = data[data['is_organic'] == 1]
minority_oversampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)
data_balanced = pd.concat([majority, minority_oversampled]).sample(frac=1, random_state=42).reset_index(drop=True)

X_balanced = data_balanced.drop(columns=['is_organic'])
y_balanced = data_balanced['is_organic']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced)

# Dimensionality Reduction
pca = PCA(n_components=0.95, random_state=42)  # Retain 95% variance
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Model Training and Evaluation
models = {
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=150, max_depth=15),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=150, learning_rate=0.1, max_depth=3),
    'SVM': SVC(kernel='rbf', C=1, gamma='scale', random_state=42)
}

best_accuracy = 0
best_model_name = ""
for name, model in models.items():
    model.fit(X_train_pca, y_train)
    y_pred = model.predict(X_test_pca)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy:.4f}")
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model_name = name

# Hyperparameter Tuning for the Best Model
if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [100, 150, 200],
        'max_depth': [10, 15, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    model = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy', n_jobs=-1, n_iter=10, random_state=42)
elif best_model_name == 'Gradient Boosting':
    param_grid = {
        'n_estimators': [100, 150, 200],
        'learning_rate': [0.05, 0.1, 0.2],
        'max_depth': [3, 5, 7]
    }
    model = RandomizedSearchCV(GradientBoostingClassifier(random_state=42), param_grid, cv=3, scoring='accuracy', n_jobs=-1, n_iter=10, random_state=42)
elif best_model_name == 'SVM':
    param_grid = {
        'C': [0.1, 1, 10],
        'gamma': ['scale', 'auto'],
        'kernel': ['rbf', 'poly']
    }
    model = RandomizedSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy', n_jobs=-1, n_iter=10, random_state=42)

model.fit(X_train_pca, y_train)
print("Best Hyperparameters:", model.best_params_)

# Final Evaluation
y_pred = model.best_estimator_.predict(X_test_pca)
final_accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Final Model ({best_model_name}) Accuracy: {final_accuracy:.4f}")
print("Classification Report:\n", report)

print(final_accuracy*100)